"""
LLM Requests Client - OpenAI 兼容的本地多模态推理客户端

模块路径：//utils/vlm_transportation/to_llama_server.py

本模块提供与 OpenAI API 完全兼容的流式请求接口，支持纯文本和多模态（文本+图片）推理。
专为 Windows 环境设计，自动处理路径转换和 Base64 编码，无缝对接本地部署的 LLM 服务。

核心特性：
- ✅ 严格遵循 OpenAI API v1 规范（/v1/chat/completions 端点）
- ✅ 原生支持流式响应（Server-Sent Events 格式）
- ✅ 自动处理 Windows 路径和图片 Base64 编码
- ✅ 纯文本/多模态双模式（img_pth 参数可选）
- ✅ 完善的错误处理和超时保护
- ✅ 零依赖外部 OpenAI 库（仅需 requests）

服务要求：
- 本地运行兼容 OpenAI API 的服务（如 vLLM, Ollama, LMStudio）
- 服务地址：http://127.0.0.1:8080
- 端点：/v1/chat/completions（支持 SSE 流式响应）
"""

import base64
import json
import requests
import os
from typing import Generator, Dict, Any, Optional


def llm_requests(prompt: str, img_pth: Optional[str] = None) -> Generator[Dict[str, Any], None, None]:
    """
    向本地 LLM 服务发送流式请求（OpenAI API 兼容格式）
    
    该函数模拟 openai.ChatCompletion.create(stream=True) 行为，返回符合 
    OpenAI 流式响应规范的生成器。自动处理 Windows 路径和图片编码，支持
    纯文本和多模态两种模式。
    
    参数
    ----------
    prompt : str
        必填。用户输入的文本提示（UTF-8 编码）
    
    img_pth : str or None, optional (default=None)
        可选。Windows 格式的图片绝对路径（支持 raw string）
        - 若提供有效路径：发送多模态请求（文本+图片）
        - 若为 None/空字符串：发送纯文本请求
        - 支持格式：JPEG/PNG（自动检测 MIME 类型）
    
    返回
    -------
    Generator[Dict[str, Any], None, None]
        流式响应生成器，每个元素为符合 OpenAI 规范的 chunk 字典，包含：
        - id: str (请求ID)
        - object: str ("chat.completion.chunk")
        - created: int (Unix timestamp)
        - model: str (模型名称)
        - choices: List[Dict] (响应选项列表)
            - index: int (选项索引)
            - delta: Dict (增量内容)
                - content: str (新增文本，可能为空)
            - finish_reason: str or None ("stop"/"length"/None)
    
    异常
    ------
    FileNotFoundError
        图片文件路径不存在或无效
    PermissionError
        无权读取指定图片文件
    ConnectionError
        无法连接到 127.0.0.1:8080 服务
    TimeoutError
        请求超过 30 秒超时
    RuntimeError
        HTTP 错误（如 4xx/5xx）或其他处理异常
    ValueError
        prompt 为空字符串
    
    示例
    --------
    >>> # 纯文本请求（推荐方式）
    >>> for chunk in llm_requests("Python 有什么特点？"):
    ...     if content := chunk['choices'][0].get('delta', {}).get('content'):
    ...         print(content, end='', flush=True)
    Python 是一种高级编程语言...
    
    >>> # 多模态请求（Windows 路径）
    >>> img = r"C:\Users\Alice\Pictures\cat.jpg"
    >>> for chunk in llm_requests("描述图片内容", img):
    ...     if content := chunk['choices'][0].get('delta', {}).get('content'):
    ...         print(content, end='', flush=True)
    图片中有一只橘色的猫在沙发上睡觉...
    
    >>> # 获取完整响应（非流式）
    >>> def get_response(prompt, img=None):
    ...     return ''.join(
    ...         chunk['choices'][0].get('delta', {}).get('content', '')
    ...         for chunk in llm_requests(prompt, img)
    ...     )
    >>> text = get_response("1+1=?")  # "2"
    
    注意事项
    ----------
    1. 服务必须实现标准 OpenAI SSE 格式（每行以 " " 开头，含 [DONE] 结束标记）
    2. Windows 路径建议使用 raw string: r"C:\path\to\file.jpg"
    3. 图片大小建议 <10MB（Base64 编码后体积增大 33%）
    4. 流式响应需实时处理，避免在生成器中长时间阻塞
    5. 本函数不维护对话历史，每次调用均为独立请求
    
    兼容性
    --------
    - OpenAI Python Library v1.0+ (模拟 stream=True 行为)
    - 本地服务：vLLM ≥0.3.0, Ollama ≥0.1.34, LMStudio ≥0.2.10
    - 模型要求：支持视觉输入的模型（如 llava, qwen-vl）需提供 img_pth
    """
    if not prompt or not prompt.strip():
        raise ValueError("prompt 参数不能为空")
    
    # 构建消息内容：纯文本 或 多模态
    if img_pth and img_pth.strip():
        img_pth = os.path.normpath(img_pth.strip())
        
        # 验证文件存在性（提前报错优于网络请求后报错）
        if not os.path.exists(img_pth):
            raise FileNotFoundError(f"图片文件不存在: {img_pth}")
        if not os.path.isfile(img_pth):
            raise FileNotFoundError(f"路径不是有效文件: {img_pth}")
        
        # 读取图片
        try:
            with open(img_pth, "rb") as f:
                img_bytes = f.read()
        except PermissionError:
            raise PermissionError(f"无权读取图片文件: {img_pth}")
        except Exception as e:
            raise IOError(f"读取图片失败 {img_pth}: {str(e)}")
        
        # 自动检测 MIME 类型（简化版）
        mime_type = "image/jpeg" if img_pth.lower().endswith(('.jpg', '.jpeg')) else "image/png"
        base64_image = base64.b64encode(img_bytes).decode('utf-8')
        
        # 多模态消息格式（OpenAI 规范）
        content = [
            {"type": "text", "text": prompt.strip()},
            {
                "type": "image_url",
                "image_url": {
                    "url": f"{mime_type};base64,{base64_image}"
                }
            }
        ]
    else:
        # 纯文本消息格式
        content = prompt.strip()
    
    # 构造请求（严格遵循 OpenAI API v1 规范）
    payload = {
        "model": "gpt-4-vision-preview",  # 本地服务通常忽略此字段
        "messages": [{"role": "user", "content": content}],
        "stream": True,
        "max_tokens": 1024  # 建议设置合理上限
    }
    
    headers = {"Content-Type": "application/json"}
    
    try:
        response = requests.post(
            "http://127.0.0.1:8080/v1/chat/completions",
            json=payload,
            headers=headers,
            stream=True,
            timeout=30
        )
        response.raise_for_status()
        
        # 处理 SSE 流（标准 OpenAI 格式）
        for line in response.iter_lines():
            if not line:
                continue
                
            decoded = line.decode('utf-8', errors='ignore').strip()
            
            # 跳过 SSE 控制行（:注释, event:, id:等）
            if decoded.startswith(':') or decoded.startswith(('event:', 'id:', 'retry:')):
                continue
            
            # 处理 data: 前缀（标准 OpenAI SSE 格式）
            if decoded.startswith(''):
                json_str = decoded[5:].strip()
                
                # 检测流结束
                if json_str == '[DONE]':
                    yield {
                        "id": "chatcmpl-local",
                        "object": "chat.completion.chunk",
                        "created": 0,
                        "model": "local-model",
                        "choices": [{
                            "index": 0,
                            "delta": {},
                            "finish_reason": "stop"
                        }]
                    }
                    break
                
                # 解析并验证响应格式
                try:
                    chunk = json.loads(json_str)
                    
                    # 确保最小合规结构（防御性编程）
                    if "choices" not in chunk:
                        chunk["choices"] = [{
                            "index": 0,
                            "delta": {"content": ""},
                            "finish_reason": None
                        }]
                    elif not chunk["choices"]:
                        chunk["choices"] = [{
                            "index": 0,
                            "delta": {"content": ""},
                            "finish_reason": None
                        }]
                    
                    yield chunk
                except json.JSONDecodeError:
                    # 跳过无效 JSON（保持流稳定性）
                    continue
                    
    except requests.exceptions.Timeout:
        raise TimeoutError("请求超时：服务器 127.0.0.1:8080 超过 30 秒无响应")
    except requests.exceptions.ConnectionError:
        raise ConnectionError(
            "无法连接到本地 LLM 服务 (127.0.0.1:8080)\n"
            "请确认：\n"
            "  1. 服务已启动（如 vLLM/Ollama/LMStudio）\n"
            "  2. 服务监听 8080 端口\n"
            "  3. 防火墙未阻止本地连接"
        )
    except requests.exceptions.HTTPError as e:
        raise RuntimeError(
            f"HTTP {e.response.status_code} 错误: {e.response.text[:200]}"
        )
    except Exception as e:
        raise RuntimeError(f"请求处理异常: {type(e).__name__}: {str(e)}")